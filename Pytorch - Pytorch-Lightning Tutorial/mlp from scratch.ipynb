{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp from scratch.ipynb","provenance":[],"authorship_tag":"ABX9TyO8DyJORlpXQ0FZEJe9s0W3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ySf4TneD177S","executionInfo":{"status":"ok","timestamp":1658728962142,"user_tz":-180,"elapsed":21834,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"4cf37c5c-c8d5-44fa-bc82-62be44c3951c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n"]},{"cell_type":"code","source":["from __future__ import print_function\n","import numpy as np ## For numerical python\n","np.random.seed(42)"],"metadata":{"id":"UlLrv73O2Bql","executionInfo":{"status":"ok","timestamp":1658728973410,"user_tz":-180,"elapsed":341,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# every layer will have a forward pass and backpass implementation. Let's create a main class layer which \n","# can do a forward pass .forward() and backward pass .backward()\n","class Layer:\n","  # A building block. Each layer is capable of performing two things:\n","  # - process input to get output --> output = layer.forward(input)\n","  # - propagate gradients through itself ---> grad_input = layer.backward(input, grad_output)\n","  # Some layers also have learnable parameters which they update during layer.backward()\n","\n","  def __init__(self):\n","    # Here we can initialize layer parameters (if any) and auxiliary stuff\n","    # A dummy stuff does nothing\n","    pass\n","\n","  def forward(self, input):\n","    # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n","    # A dummy layer just returns whatever it gets as input\n","    return input\n","\n","  def backward(self, input, grad_output):\n","    # Performs a backpropagation step through the layer wrt the given input \n","    # to compute loss gradients wrt input, we need to apply chain rule (backprop):\n","    # d loss / d x = (d loss / d layer) * (d layer / d x)\n","    # Luckily, we already receive \"d loss / d layer\" as input, so you only need to multiply it by d layer/ d x\n","    # If our layer has parameters (e.g. dense layers), we also need to update them here using d loss / d layer\n","    # The gradient of a dummy layer is precisely  grad_output, but we'll write it more explicitly\n","    num_units = input.shape[1]\n","    d_layer_d_input = np.eye(num_units)\n","    return np.dot(grad_output, d_layer_d_input) # chain rule"],"metadata":{"id":"GhF2ZL9E2NnH","executionInfo":{"status":"ok","timestamp":1658729108479,"user_tz":-180,"elapsed":318,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class ReLU(Layer):\n","  def __init__(self):\n","    # ReLU layer simply applies elementwise rectified linear unit to all inputs\n","    pass\n","\n","  def forward(self, input):\n","    # applies elementwise ReLU to [batch, input_units] matrix\n","    relu_forward = np.maximum(0,input)\n","    return relu_forward\n","\n","  def backward(self, input, grad_output):\n","    # compute gradient of loss wrt ReLU input\n","    relu_grad = input > 0\n","    return grad_output * relu_grad"],"metadata":{"id":"ejud0Iyn9ZCM","executionInfo":{"status":"ok","timestamp":1658729318271,"user_tz":-180,"elapsed":20,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Dense(Layer):\n","  def __init__(self, input_units, output_units, lr=0.1):\n","    # a dense layer is a layer which performs a learned affine transformation\n","    # f(x) = <W*x>  + b\n","    self.lr = lr\n","    self.weights = np.random.normal(loc=0.0, scale=np.sqrt(2/(input_units + output_units)), size=(input_units,output_units))\n","    self.biases = np.zeros(output_units)\n","\n","  def forward(self, input):\n","    # perform an affine transformation. f(x) = <W*x>  + b\n","    # input shape: [batch, input_units]\n","    # output shape: [batch, output_units]\n","    return np.dot(input, self.weights) + self.biases\n","\n","  def backwards(self, input, grad_output):\n","    # compute d f / d x = d f / d dense * d dense / d x where d dense / d x = weights transposed\n","    grad_input = np.dot(grad_output, self.weights.T)\n","\n","    # compute gradient wrt weights and biases\n","    grad_weights = np.dot(input.T, grad_output)"],"metadata":{"id":"jUTTorQG-MMm"},"execution_count":null,"outputs":[]}]}