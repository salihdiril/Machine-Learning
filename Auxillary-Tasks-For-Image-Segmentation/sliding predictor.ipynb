{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sliding predictor.ipynb","provenance":[],"authorship_tag":"ABX9TyPemiu0+a+/GIi5cITafaoX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"7e4d5b398bb24af1a8b8bf0a1107afad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_444de58be91f44449dbd17b89f88c205","IPY_MODEL_ec7197ed15e0448697eb102fa7153c54","IPY_MODEL_7e1674851b714f45afbf8a5565d9e2b8"],"layout":"IPY_MODEL_37299ec88018420db4194e84e144c96c"}},"444de58be91f44449dbd17b89f88c205":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8451a2ada9f42c0afa3ff22e9fc8e14","placeholder":"​","style":"IPY_MODEL_c02832021fc146dab6ecc24207dc01d4","value":"100%"}},"ec7197ed15e0448697eb102fa7153c54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_99368998e6e541f4a9ad08454747791c","max":87306240,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0290f985a9c746d9a10625bc21f1af2a","value":87306240}},"7e1674851b714f45afbf8a5565d9e2b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eefdd5f18a52467e80bd0a585c614985","placeholder":"​","style":"IPY_MODEL_042f60fdef9b48879c7f438c7c42c280","value":" 83.3M/83.3M [00:00&lt;00:00, 115MB/s]"}},"37299ec88018420db4194e84e144c96c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8451a2ada9f42c0afa3ff22e9fc8e14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c02832021fc146dab6ecc24207dc01d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99368998e6e541f4a9ad08454747791c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0290f985a9c746d9a10625bc21f1af2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eefdd5f18a52467e80bd0a585c614985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"042f60fdef9b48879c7f438c7c42c280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyNlWgdU_SeQ","executionInfo":{"status":"ok","timestamp":1660844047322,"user_tz":-180,"elapsed":17713,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"589e62ec-4958-4c3f-8926-b08891049f5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["!pip install pytorch-lightning\n","!pip install segmentation-models-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keOmX7Ec_XWG","executionInfo":{"status":"ok","timestamp":1660844064564,"user_tz":-180,"elapsed":13686,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"26ddc97a-d166-4628-b773-e1c48e1b2986"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.7.2-py3-none-any.whl (705 kB)\n","\u001b[K     |████████████████████████████████| 705 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.1.1)\n","Collecting pyDeprecate>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Collecting torchmetrics>=0.7.0\n","  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n","\u001b[K     |████████████████████████████████| 419 kB 62.6 MB/s \n","\u001b[?25hCollecting tensorboard>=2.9.1\n","  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n","\u001b[K     |████████████████████████████████| 5.9 MB 61.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 70.2 MB/s \n","\u001b[?25hCollecting PyYAML>=5.4\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 70.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.6)\n","Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.12.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.4.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.2.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (57.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.47.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.37.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.8.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (3.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (22.1.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.8.1)\n","Installing collected packages: fsspec, torchmetrics, tensorboard, PyYAML, pyDeprecate, pytorch-lightning\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.10.0 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-6.0 fsspec-2022.7.1 pyDeprecate-0.3.2 pytorch-lightning-1.7.2 tensorboard-2.10.0 torchmetrics-0.9.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (4.64.0)\n","Collecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 54.7 MB/s \n","\u001b[?25hCollecting pretrainedmodels==0.7.4\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 7.3 MB/s \n","\u001b[?25hCollecting efficientnet-pytorch==0.7.1\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (7.1.2)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.13.1+cu113)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12.1+cu113)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=3868786092f98a27df6d749d240561d89d4c243dc60c7b38fb0cd65917884106\n","  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=fea5c39096ee295bfbe2c25e77ceff60b35200eb6b0c82b563591b2ae1dabe3c\n","  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.7.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.0 timm-0.4.12\n"]}]},{"cell_type":"code","source":["import pytorch_lightning as pl\n","import pandas as pd\n","import cv2\n","import os \n","import torchvision\n","from torch import nn\n","from torch.utils.data import Dataset ,DataLoader, random_split\n","import numpy as np\n","import torch\n","from sklearn.model_selection import train_test_split \n","from torchvision import transforms, datasets, models\n","import matplotlib.pyplot as plt\n","import torchmetrics\n","from torchmetrics.functional import accuracy\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","from pytorch_lightning.callbacks.progress import TQDMProgressBar\n","from pytorch_lightning.loggers import CSVLogger\n","from torchvision.utils import make_grid\n","import math\n","import torch.nn.functional as F\n","from torchvision.ops import box_convert\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"],"metadata":{"id":"72vFqcxM_7yD","executionInfo":{"status":"ok","timestamp":1660844068591,"user_tz":-180,"elapsed":4052,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["cfg = {\n","  \"datasets\": \"/content/drive/MyDrive/Datasets/berat_kurar\",\n","\n","  # base path of the dataset\n","  \"dataset_path\": \"/content/drive/MyDrive/Datasets/mask-image\",\n","  \"image_path\": \"/content/drive/MyDrive/Datasets/mask-image/original\",\n","  \"mask_path\": \"/content/drive/MyDrive/Datasets/mask-image/labels\",\n","\n","  # define the path of the images and masks dataset that will be used in training\n","  \"train_image_dataset\": \"/content/drive/MyDrive/Datasets/berat_kurar/ptrain0\",\n","  \"train_mask_dataset\": \"/content/drive/MyDrive/Datasets/berat_kurar/pltrain0\",\n","  \"val_image_dataset\": \"/content/drive/MyDrive/Datasets/berat_kurar/pvalidation0\",\n","  \"val_mask_dataset\":  \"/content/drive/MyDrive/Datasets/berat_kurar/plvalidation0\",\n","  \"test_image_dataset\": \"/content/drive/MyDrive/Datasets/berat_kurar/ptest0\",\n","  \"test_mask_dataset\": \"/content/drive/MyDrive/Datasets/berat_kurar/pltest0\",\n","\n","  # determine the device to be used for training and evaluation\n","  \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","\n","  # determine if we'll be pinning memory during data loading\n","  \"pin_memory\": True if torch.cuda.is_available() else False,\n","\n","  # define size of the image\n","  \"input_image_height\": 224,\n","\t\"input_image_width\": 224,\n","\n","  # define the number of classes, input channels\n","  \"num_class\": 1,\n","  \"num_channel\": 3,\n","\n","  # define the backbone\n","  \"backbone\": \"resnet34\",\n","\n","  # define the encoder weights\n","  \"encoder_weight\": \"imagenet\",\n","\n","  # Initialize the learning rate, number of epochs to train for and the batch size\n","  \"lr\": 0.001,\n","  \"num_epochs\": 40,\n","  \"batch_size\": 64,\n","\n","  # define threshold to filter weak predictions\n","  \"threshold\": 0.5,\n","\n","  # define the path that the model checkpoints, prediction will be saved\n","  \"base_output\": \"/content/drive/MyDrive/output\",\n","  \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","}"],"metadata":{"id":"R_lhhqZi__BN","executionInfo":{"status":"ok","timestamp":1660844069133,"user_tz":-180,"elapsed":548,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class SegmentationDataset(Dataset):\n","  def __init__(self, img_paths, mask_paths, transforms):\n","    self.img_paths = img_paths\n","    self.mask_paths = mask_paths\n","    self.transforms = transforms\n","\n","  def __len__(self):\n","    return len(self.img_paths)\n","\n","  def __getitem__(self, idx):\n","    # load the image from disk, swap its channels from BGR to RGB,\n","    # and read the associated mask from disk in grayscale mode\n","    image = cv2.imread(self.img_paths[idx])\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    mask = cv2.imread(self.mask_paths[idx], 0)\n","\n","    # check to see if we are applying any transformations\n","    if self.transforms is not None:\n","      # apply the transformations to both image and its mask\n","      image = self.transforms(image)\n","      mask = self.transforms(mask)\n","\n","    return image, mask"],"metadata":{"id":"7kjsglQQAn5e","executionInfo":{"status":"ok","timestamp":1660844069135,"user_tz":-180,"elapsed":13,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from imutils import paths\n","\n","train_image_set = sorted(list(paths.list_images(cfg[\"train_image_dataset\"])))\n","train_mask_set = sorted(list(paths.list_images(cfg[\"train_mask_dataset\"])))\n","val_image_set = sorted(list(paths.list_images(cfg[\"val_image_dataset\"])))\n","val_mask_set = sorted(list(paths.list_images(cfg[\"val_mask_dataset\"])))\n","test_image_set = sorted(list(paths.list_images(cfg[\"test_image_dataset\"])))\n","test_mask_set = sorted(list(paths.list_images(cfg[\"test_mask_dataset\"])))"],"metadata":{"id":"rd-84mdWAz5s","executionInfo":{"status":"ok","timestamp":1660844092504,"user_tz":-180,"elapsed":23379,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","  transforms.ToPILImage(),\n"," \ttransforms.Resize((cfg[\"input_image_height\"],cfg[\"input_image_width\"])),\n","\ttransforms.ToTensor()])"],"metadata":{"id":"4M-SeZl1A3CS","executionInfo":{"status":"ok","timestamp":1660844092509,"user_tz":-180,"elapsed":26,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class LitDataModule(pl.LightningDataModule):\n","    def __init__(self, transforms=None):\n","      super().__init__()\n","      self.save_hyperparameters()\n","      self.transform = transforms\n","\n","    def setup(self, stage=None):\n","      self.train_dataset = SegmentationDataset(img_paths=train_image_set, mask_paths=train_mask_set, transforms=transform)\n","      self.val_dataset = SegmentationDataset(img_paths=val_image_set, mask_paths=val_mask_set, transforms=transform)\n","      self.test_dataset = SegmentationDataset(img_paths=test_image_set, mask_paths=test_mask_set, transforms=transform)\n","\n","    def train_dataloader(self):\n","      return DataLoader(self.train_dataset,\n","                      batch_size=cfg[\"batch_size\"],\n","                      shuffle=True)\n","\n","    def val_dataloader(self):\n","      return DataLoader(self.val_dataset,\n","                        batch_size=cfg[\"batch_size\"],\n","                        shuffle=False)\n","\n","    def test_dataloader(self):\n","      return DataLoader(self.test_dataset,\n","                        batch_size=cfg[\"batch_size\"],\n","                        shuffle=False)"],"metadata":{"id":"V7Hdwte2A4Bq","executionInfo":{"status":"ok","timestamp":1660844092512,"user_tz":-180,"elapsed":25,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import segmentation_models_pytorch as smp\n","\n","class LitModel(pl.LightningModule):\n","  def __init__(self):\n","    super().__init__()\n","    self.model = smp.Unet(\n","        encoder_name = cfg[\"backbone\"],\n","        encoder_weights = cfg[\"encoder_weight\"],\n","        in_channels = cfg[\"num_channel\"],\n","        classes = cfg[\"num_class\"],\n","        activation = None\n","      )\n","    \n","  def forward(self, x):\n","    self.model.eval()\n","    x = self.model(x)\n","    return x\n","\n","  def training_step(self, batch, batch_idx):\n","    x, y = batch\n","    logits = self.forward(x)\n","    loss = F.binary_cross_entropy_with_logits(logits, y)\n","    self.log(\"train_loss\", loss)\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","    x, y = batch\n","    logits = self.forward(x)\n","    loss = F.binary_cross_entropy_with_logits(logits, y)\n","    self.log(\"val_loss\", loss)\n","    return loss\n","\n","  def test_step(self, batch, batch_idx):\n","    x, y = batch\n","    logits = self.forward(x)\n","    loss = F.binary_cross_entropy_with_logits(logits, y)\n","    self.log(\"test_loss\", loss)\n","    return loss\n","     \n","  def configure_optimizers(self):\n","    return torch.optim.Adam(self.parameters(), lr=cfg[\"lr\"])"],"metadata":{"id":"ENXNUJxNA7Yd","executionInfo":{"status":"ok","timestamp":1660844093665,"user_tz":-180,"elapsed":1174,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","dm = LitDataModule()\n","dm.setup()\n","model = LitModel()\n","trainer = pl.Trainer(max_epochs=cfg[\"num_epochs\"],\n","                  accelerator=\"auto\",\n","                  devices=1 if torch.cuda.is_available() else None,\n","                  callbacks=[LearningRateMonitor(logging_interval=\"step\"),\n","                                TQDMProgressBar(refresh_rate=10),\n","                             EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["7e4d5b398bb24af1a8b8bf0a1107afad","444de58be91f44449dbd17b89f88c205","ec7197ed15e0448697eb102fa7153c54","7e1674851b714f45afbf8a5565d9e2b8","37299ec88018420db4194e84e144c96c","c8451a2ada9f42c0afa3ff22e9fc8e14","c02832021fc146dab6ecc24207dc01d4","99368998e6e541f4a9ad08454747791c","0290f985a9c746d9a10625bc21f1af2a","eefdd5f18a52467e80bd0a585c614985","042f60fdef9b48879c7f438c7c42c280"]},"id":"nGnU5bKTA-Wz","executionInfo":{"status":"ok","timestamp":1660844096244,"user_tz":-180,"elapsed":2600,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"770aa00d-ad89-4aa3-9c5a-33200188fe60"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/83.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4d5b398bb24af1a8b8bf0a1107afad"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["model = torch.load(cfg[\"base_output\"] + \"/pyimagesearch_example_lightning_testLoss0_25.pth\").to(cfg[\"device\"])"],"metadata":{"id":"40DqVYwVAVZg","executionInfo":{"status":"ok","timestamp":1660844105879,"user_tz":-180,"elapsed":9643,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[" test_imgs = os.listdir(\"/content/drive/MyDrive/Datasets/mask-image/original\")"],"metadata":{"id":"B7IGtOsbAlzk","executionInfo":{"status":"ok","timestamp":1660844201359,"user_tz":-180,"elapsed":9,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["pages = []\n","for page in test_imgs[25:]:\n","  page = os.path.join(\"/content/drive/MyDrive/Datasets/mask-image/original/\", page)\n","  pages.append(page)\n","\n","pages"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdnIb1b2B1Hr","executionInfo":{"status":"ok","timestamp":1660844309513,"user_tz":-180,"elapsed":288,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"2f32d7e6-9be4-4a45-e7b1-eadef061d993"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Datasets/mask-image/original/25.png',\n"," '/content/drive/MyDrive/Datasets/mask-image/original/27.png',\n"," '/content/drive/MyDrive/Datasets/mask-image/original/28.png',\n"," '/content/drive/MyDrive/Datasets/mask-image/original/29.png',\n"," '/content/drive/MyDrive/Datasets/mask-image/original/17.png']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["outersize=320\n","trimsize=110\n","innersize=outersize-2*trimsize\n","\n"],"metadata":{"id":"vkXwAKIuCBaB","executionInfo":{"status":"ok","timestamp":1660844372302,"user_tz":-180,"elapsed":353,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def getImageArr( img , width , height):\n","  img = cv2.resize(img, ( width , height ))\n","  img = img.astype(np.float32)\n","  img = img/255.0\n","\n","  img = np.rollaxis(img, 2, 0)\n","  return img"],"metadata":{"id":"_dyKoSmyCe1m","executionInfo":{"status":"ok","timestamp":1660844609939,"user_tz":-180,"elapsed":310,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def make_predictions(img):\n","  # set the model evaluation mode\n","  model.eval()\n","\n","  # turn off gradient tracking\n","  with torch.no_grad():\n","    # load the image from disk, swap its color channels, cast it to float data\n","    # type and scale its pixel values\n","    image = img\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = image.astype(\"float32\") / 255.0\n","\n","    # resize the image and make a copy of it for visualization\n","    image = cv2.resize(image, (224, 224))\n","    orig = image.copy()\n","\n","    # find the filename and generate the path to ground truth mask\n","    filename = image_path.split(os.path.sep)[-1]\n","    #ground_truth_path = os.path.join(cfg[\"test_mask_dataset\"], filename)\n","    ground_truth_path = os.path.join(\"/content/drive/MyDrive/Datasets/mask-image/labels\", filename)\n","    # load the ground truth segmentation mask in grayscale mode and resize it\n","    gt_mask = cv2.imread(ground_truth_path, 0)\n","    gt_mask = cv2.resize(gt_mask, (cfg[\"input_image_height\"], cfg[\"input_image_width\"]))\n","\n","    # make the channel axis to be the leading one, add a batch dimension, create a pytorch\n","    # tensor, and flash it to the current device\n","    image = np.transpose(image, (2, 0, 1))\n","    image = np.expand_dims(image, 0)\n","    image = torch.from_numpy(image).to(cfg['device'])\n","\n","    # make the prediction, pass the results through the sigmoid function\n","    # and convert the result to a numpy array\n","    pred_mask = model(image).squeeze()\n","    pred_mask = torch.sigmoid(pred_mask)\n","    pred_mask = pred_mask.cpu().numpy()\n","\n","    # filter out the weak predictions and convert them to integers\n","    pred_mask = (pred_mask > cfg['threshold'])*255\n","    pred_mask = pred_mask.astype(np.uint8)"],"metadata":{"id":"YdMpSpEtE8IG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(img):\n","    X = getImageArr(img ,   , args.input_height  )\n","    pr = m.predict( np.array([X]) )[0]\n","    pr = pr.reshape(( output_height ,  output_width , n_classes ) ).argmax( axis=2 )\n","    seg_img = np.zeros( ( output_height , output_width , 3  ) )\n","    for c in range(n_classes):\n","        seg_img[:,:,0] += ( (pr[:,: ] == c )*( colors[c][0] )).astype('uint8')\n","        seg_img[:,:,1] += ((pr[:,: ] == c )*( colors[c][1] )).astype('uint8')\n","        seg_img[:,:,2] += ((pr[:,: ] == c )*( colors[c][2] )).astype('uint8')\n","    seg_img = cv2.resize(seg_img  , (input_width , input_height ))\n","    return seg_img"],"metadata":{"id":"aAYU2uFnGXph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for path in pages:\n","    page=cv2.imread(path,1)\n","    rows,cols,ch=page.shape\n","    x=rows//innersize\n","    y=cols//innersize\n","    \n","    prows=(x+1)*innersize+2*trimsize\n","    pcols=(y+1)*innersize+2*trimsize\n","    ppage=np.zeros([prows,pcols,3])\n","    ppage[trimsize:rows+trimsize,trimsize:cols+trimsize,:]=page[:,:,:]\n","    pred=np.zeros([rows,cols,3])\n","    for i in range(0,prows-outersize,innersize):\n","        for j in range(0,pcols-outersize,innersize):\n","            patch=ppage[i:i+outersize,j:j+outersize,:]\n","            ppatch=predict(patch)\n","            pred[i:i+innersize,j:j+innersize,:]=ppatch[trimsize:trimsize+innersize,trimsize:trimsize+innersize,:]\n","    cv2.imwrite('predicts/'+path.split('/')[2],pred)"],"metadata":{"id":"uf7dO91LDY4x"},"execution_count":null,"outputs":[]}]}