{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQDRQfW2EUSfHqhcnutja4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIVcVT86n4tL","executionInfo":{"status":"ok","timestamp":1667363850495,"user_tz":-180,"elapsed":19786,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"e5bca8d5-4b82-409f-a8e3-d1755e579cf1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLXJoxruny_L","executionInfo":{"status":"ok","timestamp":1667363854739,"user_tz":-180,"elapsed":988,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"7d0aafd9-0499-4a30-d06c-80b7d6bfc382"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 2814k  100 2814k    0     0  7906k      0 --:--:-- --:--:-- --:--:-- 7906k\n","Archive:  data.zip\n","   creating: data/\n","  inflating: data/eng-fra.txt        \n","   creating: data/names/\n","  inflating: data/names/Arabic.txt   \n","  inflating: data/names/Chinese.txt  \n","  inflating: data/names/Czech.txt    \n","  inflating: data/names/Dutch.txt    \n","  inflating: data/names/English.txt  \n","  inflating: data/names/French.txt   \n","  inflating: data/names/German.txt   \n","  inflating: data/names/Greek.txt    \n","  inflating: data/names/Irish.txt    \n","  inflating: data/names/Italian.txt  \n","  inflating: data/names/Japanese.txt  \n","  inflating: data/names/Korean.txt   \n","  inflating: data/names/Polish.txt   \n","  inflating: data/names/Portuguese.txt  \n","  inflating: data/names/Russian.txt  \n","  inflating: data/names/Scottish.txt  \n","  inflating: data/names/Spanish.txt  \n","  inflating: data/names/Vietnamese.txt  \n"]}],"source":["# This code will download our labeled data\n","!curl -O https://download.pytorch.org/tutorial/data.zip; unzip data.zip"]},{"cell_type":"code","source":["!pip install unidecode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9WrnIhQolmp","executionInfo":{"status":"ok","timestamp":1667363975402,"user_tz":-180,"elapsed":3326,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"f54f18d0-1c76-4943-8eb1-6e40863e30fc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 4.8 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.6\n"]}]},{"cell_type":"code","source":["# Let's import our dependencies and configure some settings\n","import os\n","import random\n","from string import ascii_letters\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from unidecode import unidecode\n","\n","_ = torch.manual_seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"lmEfyAIioVNz","executionInfo":{"status":"ok","timestamp":1667363977767,"user_tz":-180,"elapsed":335,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# We first specify a directory, then try to print out all the labels there are.\n","# We can then construct a dictionary that maps a language to a numerical label\n","data_dir = \"./data/names\"\n","\n","lang2label = {\n","    file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long)\n","    for i, file_name in enumerate(os.listdir(data_dir))\n","}"],"metadata":{"id":"7FzgzWm8ofAk","executionInfo":{"status":"ok","timestamp":1667364035346,"user_tz":-180,"elapsed":299,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["lang2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYkpmUazpdGG","executionInfo":{"status":"ok","timestamp":1667364191520,"user_tz":-180,"elapsed":10,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"37401a61-fd6a-4802-cb2e-28980d11b966"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Irish': tensor([0]),\n"," 'Czech': tensor([1]),\n"," 'Vietnamese': tensor([2]),\n"," 'English': tensor([3]),\n"," 'French': tensor([4]),\n"," 'Scottish': tensor([5]),\n"," 'Korean': tensor([6]),\n"," 'Russian': tensor([7]),\n"," 'Japanese': tensor([8]),\n"," 'Italian': tensor([9]),\n"," 'Greek': tensor([10]),\n"," 'Portuguese': tensor([11]),\n"," 'Polish': tensor([12]),\n"," 'German': tensor([13]),\n"," 'Arabic': tensor([14]),\n"," 'Dutch': tensor([15]),\n"," 'Spanish': tensor([16]),\n"," 'Chinese': tensor([17])}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["\"\"\"Let’s store the number of languages in some variable so that we can use it later in our model declaration,\n","specifically when we specify the size of the final output layer\"\"\"\n","num_langs = len(lang2label)"],"metadata":{"id":"EB3D0jlLpni8","executionInfo":{"status":"ok","timestamp":1667364449327,"user_tz":-180,"elapsed":360,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\"\"\"Now, let’s preprocess the names. We first want to use unidecode to standardize all names and remove any\n","acute symbols or the likes\"\"\"\n","unidecode(\"Ślusàrski\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VNKcORbopwZt","executionInfo":{"status":"ok","timestamp":1667364505200,"user_tz":-180,"elapsed":20,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"e9068fad-7607-4cc8-9d65-1b1561af7a13"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Slusarski'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["\"\"\"Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. \n","This can first be done by constructing a char2idx mapping, as shown below\"\"\"\n","char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n","num_letters = len(char2idx)\n","num_letters"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeHpMAb0q0OR","executionInfo":{"status":"ok","timestamp":1667364596154,"user_tz":-180,"elapsed":19,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"bb27c01e-e95e-44d3-ad61-a02c5a98515e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["59"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‘. This also means that each name will now be expressed as a tensor of size (num_char, 59); in other words, each character will be a tensor of size (59,)`. We can now build a function that accomplishes this task, as shown below:"],"metadata":{"id":"jBkQu0Zhru44"}},{"cell_type":"code","source":["def name2tensor(name):\n","    tensor = torch.zeros(len(name), 1, num_letters)\n","    for i, char in enumerate(name):\n","        tensor[i][0][char2idx[char]] = 1\n","    return tensor"],"metadata":{"id":"n5CGBGgPrSWO","executionInfo":{"status":"ok","timestamp":1667364768013,"user_tz":-180,"elapsed":330,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["If you read the code carefully, you’ll realize that the output tensor is of size (num_char, 1, 59), which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size (seq_len, batch_size, input_size)."],"metadata":{"id":"LIte2gHZsiLo"}},{"cell_type":"markdown","source":["Let’s quickly verify the output of the name2tensor() function with a dummy input."],"metadata":{"id":"ibKMqwxhtA7V"}},{"cell_type":"code","source":["name2tensor(\"abc\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyGhkbM_sGdS","executionInfo":{"status":"ok","timestamp":1667365066116,"user_tz":-180,"elapsed":28,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"5f028050-90f7-4ba2-87a7-5b0419808821"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","          0., 0., 0., 0., 0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Now we need to build a our dataset with all the preprocessing steps. Let’s collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example german.txt."],"metadata":{"id":"fq9Appg5tKaq"}},{"cell_type":"code","source":["tensor_names = []\n","target_langs = []\n","\n","for file in os.listdir(data_dir):\n","    with open(os.path.join(data_dir, file)) as f:\n","        lang = file.split(\".\")[0]\n","        names = [unidecode(line.rstrip()) for line in f]\n","        for name in names:\n","            try:\n","                tensor_names.append(name2tensor(name))\n","                target_langs.append(lang2label[lang])\n","            except KeyError:\n","                pass"],"metadata":{"id":"hUdSy-WEs9Iq","executionInfo":{"status":"ok","timestamp":1667365204810,"user_tz":-180,"elapsed":1383,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["We could wrap this in a PyTorch Dataset class, but for simplicity sake let’s just use a good old for loop to feed this data into our model. Since we are dealing with normal lists, we can easily use sklearn’s train_test_split() to separate the training data from the testing data."],"metadata":{"id":"VVftcF6jwyML"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_idx, test_idx = train_test_split(\n","    range(len(target_langs)), \n","    test_size=0.1, \n","    shuffle=True, \n","    stratify=target_langs\n",")\n","\n","train_dataset = [\n","    (tensor_names[i], target_langs[i])\n","    for i in train_idx\n","]\n","\n","test_dataset = [\n","    (tensor_names[i], target_langs[i])\n","    for i in test_idx\n","]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmAf80URv1zr","executionInfo":{"status":"ok","timestamp":1667366082420,"user_tz":-180,"elapsed":2136,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"0a283604-9c51-4857-8ad2-90c710b0a47a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:746: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  array = np.asarray(array, order=order, dtype=dtype)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:746: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  array = np.asarray(array, order=order, dtype=dtype)\n"]}]},{"cell_type":"markdown","source":["Let’s see how many training and testing data we have. Note that we used a test_size of 0.1."],"metadata":{"id":"E0qBa3pvw-sQ"}},{"cell_type":"code","source":["print(f\"Train: {len(train_dataset)}\")\n","print(f\"Test: {len(test_dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5igMc4qzw0xG","executionInfo":{"status":"ok","timestamp":1667366132114,"user_tz":-180,"elapsed":22,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"3f74cd11-8b76-4271-e782-f92c4809dbe1"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Train: 18063\n","Test: 2007\n"]}]},{"cell_type":"markdown","source":["Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation."],"metadata":{"id":"2c-GDBVpxMLh"}},{"cell_type":"code","source":["class MyRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(MyRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n","    \n","    def forward(self, x, hidden_state):\n","        combined = torch.cat((x, hidden_state), 1)\n","        hidden = torch.sigmoid(self.in2hidden(combined))\n","        output = self.in2output(combined)\n","        return output, hidden\n","    \n","    def init_hidden(self):\n","        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n"],"metadata":{"id":"TY1BCsltxBas","executionInfo":{"status":"ok","timestamp":1667366289406,"user_tz":-180,"elapsed":312,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["We call init_hidden() at the start of every new batch. For easier training and learning, I decided to use kaiming_uniform_() to initialize these hidden states.\n","\n","We can now build our model and start training it."],"metadata":{"id":"G_K4D1x_xt04"}},{"cell_type":"code","source":["hidden_size = 256\n","learning_rate = 0.001\n","\n","model = MyRNN(num_letters, hidden_size, num_langs)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"_-VyGImXxnwR","executionInfo":{"status":"ok","timestamp":1667366368770,"user_tz":-180,"elapsed":358,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn’t want to cook my 13-inch MacBook Pro so I decided to stop at two epochs."],"metadata":{"id":"cRW2FOpyyGj8"}},{"cell_type":"code","source":["num_epochs = 2\n","print_interval = 3000\n","\n","for epoch in range(num_epochs):\n","    random.shuffle(train_dataset)\n","    for i, (name, label) in enumerate(train_dataset):\n","        hidden_state = model.init_hidden()\n","        for char in name:\n","            output, hidden_state = model(char, hidden_state)\n","        loss = criterion(output, label)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","        \n","        if (i + 1) % print_interval == 0:\n","            print(\n","                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n","                f\"Step [{i + 1}/{len(train_dataset)}], \"\n","                f\"Loss: {loss.item():.4f}\"\n","            )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxlsBK1cx7JC","executionInfo":{"status":"ok","timestamp":1667366621002,"user_tz":-180,"elapsed":100836,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"52613052-d8d4-48ef-8617-c8e4403ef02c"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/2], Step [3000/18063], Loss: 0.0230\n","Epoch [1/2], Step [6000/18063], Loss: 1.3776\n","Epoch [1/2], Step [9000/18063], Loss: 0.0006\n","Epoch [1/2], Step [12000/18063], Loss: 3.2339\n","Epoch [1/2], Step [15000/18063], Loss: 0.4707\n","Epoch [1/2], Step [18000/18063], Loss: 3.4659\n","Epoch [2/2], Step [3000/18063], Loss: 0.1831\n","Epoch [2/2], Step [6000/18063], Loss: 0.1333\n","Epoch [2/2], Step [9000/18063], Loss: 1.9100\n","Epoch [2/2], Step [12000/18063], Loss: 0.0001\n","Epoch [2/2], Step [15000/18063], Loss: 0.0237\n","Epoch [2/2], Step [18000/18063], Loss: 3.1870\n"]}]},{"cell_type":"markdown","source":["Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let’s go with that."],"metadata":{"id":"UzYHHb4ry-Nc"}},{"cell_type":"code","source":["num_correct = 0\n","num_samples = len(test_dataset)\n","\n","model.eval()\n","\n","with torch.no_grad():\n","    for name, label in test_dataset:\n","        hidden_state = model.init_hidden()\n","        for char in name:\n","            output, hidden_state = model(char, hidden_state)\n","        _, pred = torch.max(output, dim=1)\n","        num_correct += bool(pred == label)\n","\n","print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYtDnaS4ygNL","executionInfo":{"status":"ok","timestamp":1667366731114,"user_tz":-180,"elapsed":830,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"79b240a9-2b22-4655-b951-0ff892606caf"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 71.5994%\n"]}]},{"cell_type":"markdown","source":["The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something.\n","\n","Let’s see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction."],"metadata":{"id":"M1RmbOo8zbvK"}},{"cell_type":"code","source":["label2lang = {label.item(): lang for lang, label in lang2label.items()}\n","\n","def myrnn_predict(name):\n","    model.eval()\n","    tensor_name = name2tensor(name)\n","    with torch.no_grad():\n","        hidden_state = model.init_hidden()\n","        for char in tensor_name:\n","            output, hidden_state = model(char, hidden_state)\n","        _, pred = torch.max(output, dim=1)\n","    model.train()    \n","    return label2lang[pred.item()]"],"metadata":{"id":"NXmrpoIizTb8","executionInfo":{"status":"ok","timestamp":1667366836431,"user_tz":-180,"elapsed":21,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["myrnn_predict(\"hasan\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"i_zDGzaUztYw","executionInfo":{"status":"ok","timestamp":1667366915807,"user_tz":-180,"elapsed":50,"user":{"displayName":"Salih Diril","userId":"17082496140032816528"}},"outputId":"0c8b7d25-569d-451d-ca64-624e16dbf98f"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Arabic'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":[],"metadata":{"id":"COVWmHEvB72w"},"execution_count":null,"outputs":[]}]}